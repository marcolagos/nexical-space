---
title: Word Embeddings
date: '2023-10-25'
tags: ['Tranformers', 'Encoder-decoder', 'Word embeddings']
draft: true
summary: Understanding word embeddings
---

## Introduction

<TOCInline toc={props.toc} exclude="Introduction" />

The main idea is to embed words in a vector space of size $\mathbb{R}^k$ that captures the semantic distance among words. There are two main approaches to do this, which I will cover in the sections below.

1. Count-based methods
2. Prediction-based methods

### Count-based methods
